input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/postgresql.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://host.docker.internal:5432/ep_reporting"
    jdbc_user => "postgres"
    jdbc_password => "Rwanda123@"
    schedule => "* * * * *"
    statement => "
      SELECT
      al.id,
      al.ipaddress,
      al.service_name,
      al.token_id,
      al.time_to_archive_in_days,
      al.log_level,
      al.archive_strategy,
      al.performer_id,
      al.created_at AS timestamp,
      array_agg(json_build_object('key', m.metadata_type, 'value', m.content)) AS metadata,
      array_agg(json_build_object('type', a.name)) AS action
      FROM audit_log al
      LEFT JOIN metadata m ON al.id = m.audit_log_id AND m.deleted_at IS NULL
      LEFT JOIN action a ON al.id = a.audit_log_id AND a.deleted_at IS NULL
      WHERE al.deleted_at IS NULL
      GROUP BY
      al.id,
      al.ipaddress,
      al.service_name,
      al.token_id,
      al.time_to_archive_in_days,
      al.log_level,
      al.archive_strategy,
      al.performer_id,
      al.created_at
      ORDER BY al.created_at ASC
    "
    use_column_value => true
    tracking_column => "timestamp"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/usr/share/logstash/data/.logstash_jdbc_last_run"
    clean_run => false
  }
}
filter {
  mutate {
    copy => { "timestamp" => "@timestamp" }
  }
  mutate {
    rename => {
        "id" => "logId"
        "ipaddress" => "ipAddress"
        "service_name" => "serviceName"
        "token_id" => "tokenId"
        "time_to_archive_in_days" => "timeToArchiveInDays"
        "log_level" => "logLevel"
        "archive_strategy" => "archiveStrategy"
        "performer_id" => "performerId"
    }
  }
  json {
    source => "metadata"
    target => "metadataParsed"
    skip_on_invalid_json => true
  }
  if [metadataParsed] {
    ruby {
      code => "
        event.get('[metadataParsed]').each do |meta|
          if meta['isUserCreation'] == true
            begin
              user_data = JSON.parse(meta['value'])
              event.set('[performer][firstName]', user_data['firstName'])
              event.set('[performer][lastName]', user_data['lastName'])
              event.set('[performer][workEmail]', user_data['workEmail'])
              event.set('[performer][phoneNumber]', user_data['phoneNumber'])
            rescue JSON::ParserError
              # Skip invalid JSON
            end
          end
        end
      "
    }
  }
  mutate {
    add_field => {
      "metadataType" => "%{[metadataParsed][0][key]}"
    }
  }
  mutate {
    gsub => ["action.type", "^\"|\"$", ""]
  }
}
output { 
  elasticsearch { 
    hosts => ["http://elasticsearch:9200"] 
    index => "audit_logs-%{+YYYY.MM.dd}"
    document_id => "%{logId}"
  } 
  stdout { 
    codec => rubydebug 
  }
  file { 
    path => "/usr/share/logstash/logs/debug.log"
    codec => json_lines
  }
}